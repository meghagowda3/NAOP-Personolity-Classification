# -*- coding: utf-8 -*-
"""projecteval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Im5YwYM7kwoGdL0-6sxaiGQyq8i4_ZhE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data=pd.read_csv("/content/drive/My Drive/NAOP_new.csv",encoding='cp1252',error_bad_lines=False)

df_a=data.loc[data.Type=='A']
df_b=data.loc[data.Type=='B']
df_c=data.loc[data.Type=='C']
df_d=data.loc[data.Type=='D']

print(df_a,df_b,df_c,df_d)

new_data=data.iloc[:,[1,2]]

X=data.iloc[:,2]
y=data.iloc[:,1]

#rows and columns returns (rows, columns)
new_data.shape

######################## NLP:Text processing  ########################## 

##lower case
new_data['Posts']=new_data['Posts'].apply(lambda x:" ".join(x.lower() for x in x.split()))

## removing punctuations
new_data['Posts']=new_data['Posts'].str.replace('[^\w\s]','')

#removal of digits
new_data['Posts']=new_data['Posts'].str.replace('\d+', '')

import nltk
nltk.download('stopwords')

### removing all stopwords(english)....###
from nltk.corpus import stopwords

stop_words=stopwords.words('english')

new_data['Posts']=new_data['Posts'].apply(lambda x: " ".join(word for word in x.split() if word not in stop_words))

##removal of white spaces 
new_data['Posts']=new_data['Posts'].str.strip()

import nltk
nltk.download('wordnet')

####Lemmatization
from textblob import Word
new_data['Posts']=new_data['Posts'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

##to remove links in text data
new_data['Posts']=new_data['Posts'].replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True)

########...finding Correlation in the data....###
corrmat = new_data.corr()
print(corrmat)

## ....Finding most common occuring words in Corpus...##
posts_str=" ".join(new_data.Posts)
text=posts_str.split()

from collections import Counter
counter= Counter(text)
top_100= counter.most_common(100)
print(top_100)

###########Model_Building####################################
 ###Logistic_Regression

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

tv=TfidfVectorizer()

X=new_data.iloc[:,1]
y=new_data.iloc[:,0]

X=tv.fit_transform(new_data.Posts)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

model=LogisticRegression()
model.fit(X_train,y_train)

y_pred=model.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))

###############################################################################
#########Applying SMOTE to overcome class imbalance problem####################

from imblearn.over_sampling import SMOTE

sm=SMOTE(random_state=444)

X_train_res, y_train_res = sm.fit_resample(X_train,y_train)
X_train_res.shape
y_train_res.shape
X_test.shape
y_test.shape

model7=LogisticRegression()
model7.fit(X_train_res,y_train_res)

y_pred7=model7.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(accuracy_score(y_test,y_pred7))
print(classification_report(y_test,y_pred7))
print(confusion_matrix(y_test,y_pred7))

########### MODEL EVALUATION after SMOTE ##########
########## K-FOLD-CROSS-validation ####

from sklearn import model_selection
from sklearn.linear_model import LogisticRegression

kfold = model_selection.KFold(n_splits=10, random_state=100)

model_kfold = LogisticRegression()
results_kfold = model_selection.cross_val_score(model_kfold, X_test, y_test, cv=kfold)

print("Accuracy: %.2f%%" % (results_kfold.mean()*100.0))

######################################### XGBOOST after applying smote ######################################

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
from xgboost import plot_importance

from sklearn.model_selection import train_test_split

X_train_res, X_test, y_train_res, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

import xgboost as xgb
from xgboost import XGBClassifier

xgb = XGBClassifier(objective='multi:softmax',
                       num_class=4,
                      booster='gbtree', 
                      colsample_bytree=0.8,
                      max_depth=3,
                      min_child_weight=7,
                      n_estimators=1000, n_jobs=-1, 
                      random_state=0, reg_alpha=0, subsample=0.6)

xgb.fit(X_train_res,y_train_res,eval_set=[(X_train_res,y_train_res),(X_test, y_test)],
          early_stopping_rounds=100,verbose=200)

y_pred10=xgb.predict(X_test)

accuracy = accuracy_score(y_test, y_pred10)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

print(accuracy_score(y_test,y_pred10))
print(confusion_matrix(y_test,y_pred10))
print(classification_report(y_test,y_pred10))

#########################kfold on smote and model as logistic###########
from sklearn.model_selection import KFold
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression

kf = KFold(n_splits=5)
kf

for fold, (train_index, test_index) in enumerate(kf.split(X),1):
    X_train = X[train_index]
    y_train = y[train_index]  
    X_test = X[test_index]
    y_test = y[test_index] 
    sm = SMOTE()
    X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)
model= LogisticRegression()
model.fit(X_train_oversampled, y_train_oversampled )
y_pred = model.predict(X_test)


print(f'For fold {fold}:')
print(f'Accuracy: {model.score(X_test, y_test)}')
print(f'f-score: {f1_score(y_test, y_pred,average="weighted")}')